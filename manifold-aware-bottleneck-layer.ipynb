{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b58458",
   "metadata": {},
   "source": [
    "# Squeezing the Noise: Why JiT’s 128-Dimension Bottleneck is a Mathematical Filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9528431",
   "metadata": {},
   "source": [
    "## The Hook: The \"Impossible\" Task\n",
    "Start by explaining the common wisdom: \"You can't do diffusion in pixel space because raw pixels are too noisy and high-dimensional. That's why we use VAEs.\" Then, introduce the JiT paper's \"Back to Basics\" approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004797d",
   "metadata": {},
   "source": [
    "## The Core Analysis: Dimensions of Reality vs. Dimensions of Noise\n",
    "This is where you drop your table. You should frame it as a \"Dimensional Efficiency\" comparison.\"In a $16 \\times 16$ image patch, there are 768 mathematical degrees of freedom. But how many of them actually matter? Our spectral analysis on ImageNet shows that 95% of the 'visual story' is told in just 34 dimensions. In contrast, random white noise requires 702 dimensions to explain that same 95%.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d870d",
   "metadata": {},
   "source": [
    "##  The \"Spectral Gap\" Visualization\n",
    "Explain the Eigenvalue Decay you found. Use the comparison between ImageNet and CelebA to show that the \"simpler\" the data (like aligned faces), the more aggressive the bottleneck can be.\n",
    "\n",
    "ImageNet: 99.9% variance at 377 dims.\n",
    "\n",
    "CelebA: 99.9% variance at 189 dims.\n",
    "\n",
    "The 128-Dim \"Sweet Spot\": Explain that at 128, you are essentially \"buying\" a massive reduction in noise (blocking 83% of noise dimensions) while only \"paying\" with a tiny 1-2% loss in visual variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bdd2d3",
   "metadata": {},
   "source": [
    "## Why the Bottleneck Enables $x$-Prediction\n",
    "\n",
    "This is the most technical (and important) part.In standard diffusion, we predict noise ($\\epsilon$). In JiT, we predict the clean image ($x$).The Problem: Predicting $x$ from pure noise is an \"ill-posed\" problem—there are too many ways to map noise to a cat.The Solution: The bottleneck acts as a structural prior. It tells the model: \"Don't even try to see the high-frequency jitters. Only look at the low-frequency manifold.\" By truncating the noise, the bottleneck makes the mapping from noisy input to clean output much \"smoother\" and easier for the Transformer to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f790eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Conclusion: Beyond the VAE\n",
    "Wrap up by arguing that the 128-dim bottleneck is essentially a \"Linear VAE\" that is learned on-the-fly. It proves that we don't necessarily need complex, pre-trained latent spaces if we understand the spectral geometry of our data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
